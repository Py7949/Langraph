{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qAcLFzyX6LYy"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"bigscience/bloomz-560m\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=200\n",
        ")\n"
      ],
      "metadata": {
        "id": "Gu1Z6vX16NTY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a state\n",
        "class LLMState(TypedDict):\n",
        "    question: str\n",
        "    answer: str"
      ],
      "metadata": {
        "id": "KGESyWh37Z24"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_qa(state: LLMState)->LLMState:\n",
        "   # extract the question from state\n",
        "    question = state[\"question\"]\n",
        "\n",
        "   # form a prompt\n",
        "    prompt = f'Answer the following question: {question}'\n",
        "    try:\n",
        "        answer = llm.invoke(prompt)\n",
        "        if not answer:\n",
        "            answer = \"[No response received from Hugging Face]\"\n",
        "    except Exception as e:\n",
        "        answer = f\"[Error from Hugging Face: {e}]\"\n",
        "\n",
        "    state[\"answer\"] = answer\n",
        "    return state\n",
        "\n",
        "   # ask that question to the LLM\n",
        "    #answer =llm.invoke(prompt)\n",
        "\n",
        "\n",
        "    # update the answer in the state\n",
        "    #state['answer'] = answer\n",
        "\n",
        "    #return state"
      ],
      "metadata": {
        "id": "G-CjSfjs7aVT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(LLMState)"
      ],
      "metadata": {
        "id": "wLuZD0xx9TAO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add node\n",
        "graph.add_node('llm_qa', llm_qa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZx_AFDv76CL",
        "outputId": "23a8e454-04a7-4a3e-8be9-c31226e32425"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7aeafaa37790>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add edge\n",
        "graph.add_edge(START, 'llm_qa')\n",
        "graph.add_edge('llm_qa', END)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ3aUvV29qkD",
        "outputId": "3b56c91b-a310-45d9-be15-9f1d050e15e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7aeafaa37790>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile\n",
        "workflow = graph.compile()"
      ],
      "metadata": {
        "id": "Sri3NNhF9t14"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute\n",
        "initial_state = {\"question\": \"How far is moon from the earth?\"}\n",
        "final_state = workflow.invoke(initial_state)\n",
        "\n",
        "print(\"Q:\", final_state[\"question\"])\n",
        "print(\"A:\", final_state[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGZGDNa792C-",
        "outputId": "ec14de01-08ce-4a0b-a202-106287ddfe43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: How far is moon from the earth?\n",
            "A: [Error from Hugging Face: ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A90rRRg7-Cpq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agrhxMU1-wDX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8iIwaZyNQTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}